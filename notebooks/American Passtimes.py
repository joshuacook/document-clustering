# Databricks notebook source
from numpy.random import seed
seed(1010)

# COMMAND ----------

# MAGIC %md ## Implementation in Scikit-Learn
# MAGIC 
# MAGIC ![](https://www.evernote.com/l/AAGiYGcKcIxIaJ7sCg97K9JDtUO2dY9mywoB/image.png )

# COMMAND ----------

# MAGIC %md ### Raw Text Data
# MAGIC 
# MAGIC <img src="https://www.evernote.com/l/AAFfAyDQQ1xGPLTIxT2hcUSLrHuQDbYzsuYB/image.png" width=600px>
# MAGIC 
# MAGIC Here each line of text is a **document** and the collection of all lines of text is the **body**.

# COMMAND ----------

# MAGIC %sh curl --remote-name-all 'https://joshua-databricks.s3-us-west-2.amazonaws.com/text-data/pagesParsed.json'

# COMMAND ----------

dbutils.fs.cp("file:/databricks/driver/pagesParsed.json", "dbfs:/FileStore/tmp/pagesParsed.json")

# COMMAND ----------

wikiDF = spark.read.json("/FileStore/tmp/pagesParsed.json")

wiki_df = wikiDF.toPandas()
display(wiki_df) 

# COMMAND ----------

# MAGIC %md ### Document-Term Matrix
# MAGIC 
# MAGIC <img src="https://www.evernote.com/l/AAFtjaKOjT5CYr5N_NPHKU6vpBWNnBgbWLIB/image.png" width=600px>
# MAGIC 
# MAGIC The Document-Term Matrix can be created using the `TfidfVectorizer` model [[doc]](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) in Scikit-Learn.

# COMMAND ----------

import re
from sklearn.feature_extraction.text import TfidfVectorizer

def no_number_preprocessor(tokens):
    r = re.sub('(\d)+', '', tokens.lower())
    # This alternative just removes numbers:
    # r = re.sub('(\d)+', '', tokens.lower())
    return r
  
vectorizer = TfidfVectorizer(stop_words='english', preprocessor=no_number_preprocessor)
bag_of_words = vectorizer.fit_transform(wiki_df.text)

# COMMAND ----------

# MAGIC %md ### Singular Value Decomposition
# MAGIC 
# MAGIC <img src="https://www.evernote.com/l/AAEhTiOBufhPwKBx-Hgufx4XZ5XyfsCp8cMB/image.png" width=600px>
# MAGIC 
# MAGIC This can be achieved using the `TruncatedSVD` model. 
# MAGIC 
# MAGIC The function is named "truncated" SVD because it is capable of returning a dataset with fewer features than it is passed without significant loss of information, that is, it is great for reducing the dimension of data.

# COMMAND ----------

from sklearn.decomposition import TruncatedSVD

svd = TruncatedSVD(n_components=2)
lsa = svd.fit_transform(bag_of_words)

# COMMAND ----------

# MAGIC %md ### Topic Encoded Data
# MAGIC 
# MAGIC <img src="https://www.evernote.com/l/AAGhSgfs1nZHAIYfbnmNaHU8YjMV2i9fTmgB/image.png" width=600px>
# MAGIC 
# MAGIC The process transforms the original data into **topic-encoded data**.
# MAGIC 
# MAGIC Here, each row is indexed by its original text value. The data now consists of two columns of data one representing each of the two topics used to encode the **body**. Recall that this value of 2 was passed as an argument to the `TruncatedSVD` in the previous step. 

# COMMAND ----------

import pandas as pd

topic_encoded_df = pd.DataFrame(lsa, columns = ["topic_1", "topic_2"])
topic_encoded_df["title"] = wiki_df.title
topic_encoded_df["is_baseball"] = wiki_df.category == "Baseball"
display(topic_encoded_df[["title", "topic_1", "topic_2", "is_baseball"]])

# COMMAND ----------

# MAGIC %md ## Byproducts of the Latent Semantic Analysis
# MAGIC 
# MAGIC The LSA generates a few byproducts that are useful for analysis:
# MAGIC 
# MAGIC - the **dictionary** or the set of all words that appear at least once in the **body**
# MAGIC - the **encoding matrix** used to encode the documents into topics. The **encoding matrix** can be studied to gain an understanding of the **topics** that are latent to the **body**. 

# COMMAND ----------

# MAGIC %md #### The Dictionary
# MAGIC 
# MAGIC The dictionary is an attribute of a fit `TfidfVectorizer` model and can be accessed using the `.get_feature_names` method.

# COMMAND ----------

import random 
dictionary = vectorizer.get_feature_names()
for _ in range(10):
  print(random.choice(dictionary))

# COMMAND ----------

# MAGIC %md #### The Encoding Matrix
# MAGIC 
# MAGIC The **encoding matrix** is comprised of the `components_` stored as an attribute of a fit `TruncatedSVD`. We can examine this matrix to gain an understanding of the **topics** latent to the **body**.
# MAGIC 
# MAGIC **Note:** in `sklearn`, attributes of a model that are generated by a fitting process have a trailing underscore in their name as can be seen here with `svd.components_`. 

# COMMAND ----------

encoding_matrix = pd.DataFrame(svd.components_,
                               index=['topic_1', 'topic_2'],
                               columns=dictionary).T
encoding_matrix["dictionary"] = dictionary
display(encoding_matrix.head())

# COMMAND ----------

# MAGIC %md #### Interpret The Encoding Matrix
# MAGIC 
# MAGIC What are the top words for each topic? What dimensions in word-space explain most of the variance in the data? 
# MAGIC 
# MAGIC To analyze this, we will need to look at the *absolute value* of the expression of each word in the topic. 

# COMMAND ----------

display(encoding_matrix.sort_values('topic_1', ascending=False).head(20))

# COMMAND ----------

display(encoding_matrix.sort_values('topic_2', ascending=False).head(20))

# COMMAND ----------

# MAGIC %md ### Plot Topic Encoded Data

# COMMAND ----------

import matplotlib.pyplot as plt

fig, ax = plt.subplots()

for val in topic_encoded_df.is_baseball.unique():
  topic_1 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_1'].values
  topic_2 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_2'].values
  print(val)
  color = "red" if val else "green"
  label = "Baseball" if val else "Jazz"
  ax.scatter(topic_1, topic_2, c=color, alpha=0.3, label=label)
# made the colors represent different books

ax.set_xlabel('First Topic')
ax.set_ylabel('Second Topic')
ax.axvline(linewidth=0.5)
ax.axhline(linewidth=0.5)
ax.legend()

display(fig)

# COMMAND ----------

topic_encoded_df.columns

# COMMAND ----------

# MAGIC %md ## Document Clustering via Gaussian Mixture Model

# COMMAND ----------

fig, ax = plt.subplots(1,2,figsize=(20,6))

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=2)
gmm.fit(topic_encoded_df[["topic_1", "topic_2"]])
labels = gmm.predict(topic_encoded_df[["topic_1", "topic_2"]])

# COMMAND ----------

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,2,figsize=(20,10))

for val in topic_encoded_df.is_baseball.unique():
  topic_1 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_1'].values
  topic_2 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_2'].values
  print(val)
  color = "red" if val else "green"
  label = "Baseball" if val else "Jazz"
  ax[0].scatter(topic_1, topic_2, c=color, alpha=0.3, label=label)
  
topic_encoded_df.plot(kind="scatter", x="topic_1", y="topic_2", c=labels, ax=ax[1])  
# made the colors represent different books

ax[0].set_xlabel('First Topic')
ax[0].set_ylabel('Second Topic')
ax[0].axvline(linewidth=0.5)
ax[0].axhline(linewidth=0.5)
ax[0].legend()

display(fig)

# COMMAND ----------

# MAGIC %md ## Revised Topic Model

# COMMAND ----------

svd = TruncatedSVD(n_components=100)
lsa = svd.fit_transform(bag_of_words)

# COMMAND ----------

import numpy as np
fig, ax = plt.subplots(1,2, figsize=(20,4))
ax[0].plot(svd.explained_variance_ratio_)
ax[1].plot(np.diff(svd.explained_variance_))
display(fig)

# COMMAND ----------

import numpy as np
fig, ax = plt.subplots(1,2, figsize=(20,4))
ax[0].plot(svd.explained_variance_ratio_[:7])
ax[1].plot(np.diff(svd.explained_variance_)[:7])
display(fig)

# COMMAND ----------

svd = TruncatedSVD(n_components=20)
lsa = svd.fit_transform(bag_of_words)

# COMMAND ----------

encoding_matrix = pd.DataFrame(svd.components_,
                               index=['topic_' + str(i) for i in range(1,21)],
                               columns=dictionary).T
encoding_matrix["dictionary"] = dictionary
display(encoding_matrix.head())

# COMMAND ----------

list(encoding_matrix.sort_values('topic_1', ascending=False).head(20).dictionary)

# COMMAND ----------

list(encoding_matrix.sort_values('topic_2', ascending=False).head(20).dictionary)

# COMMAND ----------

list(encoding_matrix.sort_values('topic_3', ascending=False).head(20).dictionary)

# COMMAND ----------

list(encoding_matrix.sort_values('topic_4', ascending=False).head(20).dictionary)

# COMMAND ----------

list(encoding_matrix.sort_values('topic_5', ascending=False).head(20).dictionary)

# COMMAND ----------

list(encoding_matrix.sort_values('topic_6', ascending=False).head(20).dictionary)

# COMMAND ----------

twenty_topics = ['topic_' + str(i) for i in range(2,21)] # note that topic 1 has been removed

# COMMAND ----------

topic_encoded_df = pd.DataFrame(lsa[:, 1:], columns = twenty_topics)
topic_encoded_df["text"] = wiki_df.text
topic_encoded_df["title"] = wiki_df.title
topic_encoded_df["is_baseball"] = wiki_df.category == "Baseball"

# COMMAND ----------

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=2)
gmm.fit(topic_encoded_df[twenty_topics])
labels = gmm.predict(topic_encoded_df[twenty_topics])

# COMMAND ----------

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,2,figsize=(20,10))

for val in topic_encoded_df.is_baseball.unique():
  topic_1 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_2'].values
  topic_2 = topic_encoded_df[topic_encoded_df.is_baseball == val]['topic_3'].values
  print(val)
  color = "red" if val else "green"
  label = "Baseball" if val else "Jazz"
  ax[0].scatter(topic_1, topic_2, c=color, alpha=0.3, label=label)
  
topic_encoded_df.plot(kind="scatter", x="topic_2", y="topic_3", c=labels, ax=ax[1])
# made the colors represent different books

ax[0].set_xlabel('First Topic')
ax[0].set_ylabel('Second Topic')
ax[0].axvline(linewidth=0.5)
ax[0].axhline(linewidth=0.5)
ax[0].legend()

display(fig)

# COMMAND ----------

fig, ax = plt.subplots(figsize=(4,4))
topic_encoded_df[labels==0].plot(kind="scatter", x="topic_2", y="topic_3", ax=ax)
display(fig)

# COMMAND ----------

jazz = topic_encoded_df[labels==0]

# COMMAND ----------

vectorizer_jazz = TfidfVectorizer(stop_words='english', preprocessor=no_number_preprocessor)
bag_of_words_jazz = vectorizer_jazz.fit_transform(jazz.text)
svd_jazz = TruncatedSVD(n_components=20)
lsa_jazz = svd_jazz.fit_transform(bag_of_words_jazz)
encoding_matrix_jazz = pd.DataFrame(svd_jazz.components_,
                               index=['topic_' + str(i) for i in range(1,21)],
                               columns=vectorizer_jazz.get_feature_names()).T
encoding_matrix_jazz["dictionary"] = vectorizer_jazz.get_feature_names()
list(encoding_matrix_jazz.sort_values('topic_1', ascending=False).head(5).dictionary)

gmm = GaussianMixture(n_components=2)
gmm.fit(jazz[twenty_topics])
jazz_labels = gmm.predict(jazz[twenty_topics])

# COMMAND ----------

import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,1,figsize=(10,6))

jazz.plot(kind="scatter", x="topic_2", y="topic_3", c=jazz_labels, ax=ax)
# made the colors represent different books

ax.set_xlabel('First Topic')
ax.set_ylabel('Second Topic')
ax.axvline(linewidth=0.5)
ax.axhline(linewidth=0.5)
ax.legend()

display(fig)

# COMMAND ----------

jazz_group_1 = jazz[jazz_labels == 0]
jazz_group_2 = jazz[jazz_labels == 1]

# COMMAND ----------

display(jazz_group_1[["title", "text"]].head())

# COMMAND ----------

vectorizer_1 = TfidfVectorizer(stop_words='english', preprocessor=no_number_preprocessor)
vectorizer_2 = TfidfVectorizer(stop_words='english', preprocessor=no_number_preprocessor)
bag_of_words_jazz_1 = vectorizer_1.fit_transform(jazz_group_1.text)
bag_of_words_jazz_2 = vectorizer_2.fit_transform(jazz_group_2.text)

# COMMAND ----------

svd_1 = TruncatedSVD(n_components=20)
lsa = svd_1.fit_transform(bag_of_words_jazz_1)
encoding_matrix_1 = pd.DataFrame(svd_1.components_,
                               index=['topic_' + str(i) for i in range(1,21)],
                               columns=vectorizer_1.get_feature_names()).T
encoding_matrix_1["dictionary"] = vectorizer_1.get_feature_names()
list(encoding_matrix_1.sort_values('topic_1', ascending=False).head(5).dictionary)

# COMMAND ----------

svd_2 = TruncatedSVD(n_components=20)
lsa = svd_2.fit_transform(bag_of_words_jazz_2)
encoding_matrix_2 = pd.DataFrame(svd_2.components_,
                               index=['topic_' + str(i) for i in range(1,21)],
                               columns=vectorizer_2.get_feature_names()).T
encoding_matrix_2["dictionary"] = vectorizer_2.get_feature_names()
list(encoding_matrix_2.sort_values('topic_1', ascending=False).head(5).dictionary)

# COMMAND ----------

print(list(encoding_matrix_1.sort_values('topic_1', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_2', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_3', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_4', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_5', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_6', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_7', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_8', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_9', ascending=False).head(5).dictionary))
print(list(encoding_matrix_1.sort_values('topic_10', ascending=False).head(5).dictionary))

# COMMAND ----------

print(list(encoding_matrix_2.sort_values('topic_1', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_2', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_3', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_4', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_5', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_6', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_7', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_8', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_9', ascending=False).head(5).dictionary))
print(list(encoding_matrix_2.sort_values('topic_10', ascending=False).head(5).dictionary))

# COMMAND ----------
